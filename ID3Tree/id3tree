import csv
from sklearn.utils import shuffle
from numpy.random import random_integers
import numpy as np


class ID3Tree:
    def __init__(self):
        pass

    def calc_entropy(self, p):
        if p != 0:
            return -p * np.log2(p)
        else:
            return 0

    def calc_info_gain(self, data, classes, feature):
        gain = 0
        nData = len(data)
        # list the values that feature can take
        values = []
        for dpoint in data:
            if dpoint in data:
                if dpoint[feature] not in values:
                    values.append(dpoint[feature])

        featureCounts = np.zeros(len(values))
        entropy = np.zeros(len(values))
        valueIndex = 0

        # Find where those values appear in data[feature] and the corresponding class
        for v in values:
            dataIndex = 0
            newClasses = []
            for dpoint in data:
                if dpoint[feature] == v:
                    featureCounts[valueIndex] += 1
                    newClasses.append(classes[dataIndex])
                dataIndex += 1

            # get the values in newClasses
            classValues = []
            for aClass in newClasses:
                if classValues.count(aClass) == 0:
                    classValues.append(aClass)
            classCounts = np.zeros(len(classValues))
            classIndex = 0

            for classVal in classValues:
                for aClass in newClasses:
                    if aClass == classVal:
                        classCounts[classIndex] += 1


                classIndex += 1

            for classIndex in range(len(classValues)):
                entropy[valueIndex] += self.calc_entropy(float(classCounts[classIndex])/sum(classCounts))
            gain += float(featureCounts[valueIndex])/nData * entropy[valueIndex]
            valueIndex += 1
        return gain

    if __name__ == '__main__':
        def loadDataset(self, filename, split, trainingData, trainingTarget, testData, testTarget):
            # open it up and read it into a file
            f = open(filename)
            lines = csv.reader(f)
            dataset = list(lines)
            f.close()

            #time to randomize the data...
            dataset = shuffle(dataset, random_state = random_integers(1,1000000))

            #now divy up the file according to the file name
            if filename == "votes.csv":
                for i in range(len(dataset)):
                    if i < split * len(dataset):
                        trainingTarget.append(dataset[i][0])
                        trainingData.append(dataset[i][1:])
                    else:
                        testTarget.append(dataset[i][0])
                        testData.append(dataset[i][1:])
            elif filename == "iris.csv":
                for i in range(len(dataset)):
                    if i < split * len(dataset):
                        trainingTarget.append(dataset[i][:4])
                        trainingData.append(dataset[i][4])
                    else:
                        testTarget.append(dataset[i][:4])
                        testData.append(dataset[i][4])
            elif filename == "lenses.csv":
                pass

    def make_tree(self, data, target, column):
        amtOfData = len(data)
        numOfFeatures = len(data[0])

        # from the text not sure what exactly this is for... ?
        try:
            self.column
        except:
            self.column = column

        # List the possible classes
        newTargets = []
        for aclass in target:
            if newTargets.count(aclass) == 0:
                newTargets.append(aclass)

        # Compute the default class (and total entropy)
        frequency = np.zeros(len(newTargets))

        totalEntropy = 0
        index = 0
        for aclass in newTargets:
            frequency[index] = target.count(aclass)
            totalEntropy += self.calc_entropy(float(frequency[index]) / amtOfData)
            index += 1
        default = target[np.argmax(frequency)]
        if amtOfData == 0 or numOfFeatures == 0:
            # empty branch
            return default
        elif target.count(target[0]) == amtOfData:
            # only 1 class left
            return target[0]
        else:
            # choose best feature
            gain = np.zeros(numOfFeatures)
            for feature in range(numOfFeatures):
                g = self.calc_info_gain(data, target, feature)
                gain[feature] = totalEntropy - g
            bestFeature = np.argmax(gain)
            tree = {column[bestFeature]: {}}
            values = []

            for dp in data:
                thingy = dp[bestFeature]
                if dp[bestFeature] not in values:
                    values.append(dp[bestFeature])

            for value in values:
                newData = []
                newTargets = []
                index = 0
                for dp in data:
                    if dp[bestFeature] == value:
                        if bestFeature == 0:
                            newdatapoint = dp[1:]
                            newColumns = column[1:]
                        elif bestFeature == numOfFeatures:
                            newdatapoint = dp[:-1]
                            newColumns = column[:-1]
                        else:
                            newdatapoint = dp[:bestFeature]
                            newdatapoint.extend(dp[bestFeature + 1:])
                            newColumns = column[:bestFeature]
                            newColumns.extend(column[bestFeature + 1:])
                        newData.append(newdatapoint)
                        newTargets.append(target[index])
                    index += 1

                # Now recurse to the next level
                subtree = self.make_tree(newData, newTargets, newColumns)

                # And on returning, add the subtree on to the tree
                tree[column[bestFeature]][value] = subtree
            return tree

    def printTree(self, tree, space):
        if type(tree) == dict:
            print(space, list(tree.keys())[0])
            for item in list(tree.values())[0].keys():
                print(space, item)
                self.printTree(list(tree.values())[0][item], space + "\t")
        else:
            print(space, "\t->\t", tree)

    def testIfWorking(self, data, target, tree):
        if type(tree) == dict:
            treeKeys = list(tree.keys())[0]
            treeValues = list(tree.values())[0]
            answerToQuestion = data[treeKeys]
            subTreeKeys = treeValues.keys()
            testing = subTreeKeys[0]
            subTreeValues = subTree.values()
            for i in range(len(subTreeKeys)):
                if list(subTreeKeys.keys())[i] == answerToQuestion:
                    thingy = subTreeValues[subTreeKeys[i]]
                    pass
            print("SDLKFJSDLKFJSD")



def main(filename):
    # read in the data
    helper = ID3Tree()
    trainingData = []
    trainingTarget = []
    testData = []
    testTarget = []
    split = .7
    helper.loadDataset(filename, split, trainingData, trainingTarget, testData, testTarget)

    columns = []
    for i in range(len(trainingData[0])):
        columns.append(i)

    # make it build a tree out of the TRAIN data
    tree = helper.make_tree(trainingData, trainingTarget, columns)
    helper.printTree(tree, " ")
    print("ENTIRE TREE", tree)
    # run the test data into the tree and see if i'm right.
    for i in range(len(testData)):
        helper.testIfWorking(testData[i], testTarget[i], tree)

main('votes.csv')
