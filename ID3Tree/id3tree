import csv
import random

import numpy as np


class ID3Tree:
    def __init__(self):
        pass

    def calc_entropy(self, p):
        if p != 0:
            return -p * np.log2(p)
        else:
            return 0

    def calc_info_gain(self, data, classes, feature):
        gain = 0
        nData = len(data)
        # list the values that feature can take
        values = []
        for dpoint in data:
            if dpoint in data:
                if dpoint[feature] not in values:
                    values.append(dpoint[feature])

        featureCounts = np.zeros(len(values))
        entropy = np.zeros(len(values))
        valueIndex = 0

        # Find where those values appear in data[feature] and the corresponding class
        for v in values:
            dataIndex = 0
            newClasses = []
            for dpoint in data:
                if dpoint[feature] == v:
                    featureCounts[valueIndex] += 1
                    newClasses.append(classes[dataIndex])
                dataIndex += 1

            # get the values in newClasses
            classValues = []
            for aClass in newClasses:
                if classValues.count(aClass) == 0:
                    classValues.append(aClass)
            classCounts = np.zeros(len(classValues))
            classIndex = 0

            for classVal in classValues:
                for aClass in newClasses:
                    if aClass == classVal:
                        classCounts[classIndex] += 1


                classIndex += 1

            for classIndex in range(len(classValues)):
                entropy[valueIndex] += self.calc_entropy(float(classCounts[classIndex])/sum(classCounts))
            gain += float(featureCounts[valueIndex])/nData * entropy[valueIndex]
            valueIndex += 1
        return gain

    def loadDataset(self, filename, split, trainingSet=[], testSet=[]):
        f = open(filename)
        lines = csv.reader(f)
        dataset = list(lines)
        f.close()

        for x in range(len(dataset)):
            for y in range(len(dataset[x]) - 1):  # -1 is all but last?
                dataset[x][y] = float(dataset[x][y])
            if random.random() < split:
                trainingSet.append(dataset[x])
            else:
                testSet.append(dataset[x])

    def make_tree(self, data, classes, column):
        amtOfData = len(data)
        numOfFeatures = len(data[0])

        # from the text not sure what exactly this is for... ?
        try:
            self.column
        except:
            self.column = column

        # List the possible classes
        newClasses = []
        for aclass in classes:
            if newClasses.count(aclass) == 0:
                newClasses.append(aclass)

        # Compute the default class (and total entropy)
        frequency = np.zeros(len(newClasses))

        totalEntropy = 0
        index = 0
        for aclass in newClasses:
            frequency[index] = classes.count(aclass)
            totalEntropy += self.calc_entropy(float(frequency[index]) / amtOfData)
            index += 1
        default = classes[np.argmax(frequency)]
        if amtOfData == 0 or numOfFeatures == 0:
            # empty branch
            return default
        elif classes.count(classes[0]) == amtOfData:
            # only 1 class left
            return classes[0]
        else:
            # choose best feature
            gain = np.zeros(numOfFeatures)
            for feature in range(numOfFeatures):
                g = self.calc_info_gain(data,classes,feature)
                gain[feature] = totalEntropy - g
            bestFeature = np.argmax(gain)
            tree = {column[bestFeature]: {}}
            values = []

            for dp in data:
                if dp[feature] not in values:
                    values.append(dp[bestFeature])

            for value in values:
                newData = []
                newClasses = []
                index = 0
                for dp in data:
                    if dp[bestFeature] == value:
                        if bestFeature == 0:
                            newdatapoint = dp[1:]
                            newNames = column[1:]
                        elif bestFeature == numOfFeatures:
                            newdatapoint = dp[:-1]
                            newNames = column[:-1]
                        else:
                            newdatapoint = dp[:bestFeature]
                            newdatapoint.extend(dp[bestFeature + 1:])
                            newNames = column[:bestFeature]
                            newNames.extend(column[bestFeature + 1:])
                        newData.append(newdatapoint)
                        newClasses.append(classes[index])
                    index += 1

                    # Now recurse to the next level
                    subtree = self.make_tree(newData, newClasses, newNames)

                    # And on returning, add the subtree on to the tree
                    tree[column[bestFeature]][value] = subtree
                return tree
def main(filename):
    # read in the data
    helper = ID3Tree()
    trainingset = []
    testset = []
    split = .7
    helper.loadDataset((filename, split, trainingset, testset))
    print('Train set: ' + repr(len(trainingset)))
    print('Test set: ' + repr(len(testset)))

    # split up the data into testing/training
    # create an ID3 class thing
    # make it build a tree out of the TRAIN data
    # run the test data into the tree and see if i'm right.

main('votes.csv')
